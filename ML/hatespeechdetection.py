# -*- coding: utf-8 -*-
"""Hatespeechdetection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eVNg31ZdxR23vkZKfPzDwULxeHr7na3j

# Data fetching and Shuffling
"""

sentences=[]
labels=[]
with open("/content/Labels.txt","r") as csv_file:
  for row in csv_file:
    if row[0]=="F":
      labels.append(0)
    elif row[0]=="H":
      # print(row)
      labels.append(1)

with open("/content/Posts.txt","r") as csv_file:
  index=0
  for row in csv_file:
    sentences.append((row,index))
    index+=1
print(len(sentences),len(labels),sum(labels))

sentences_shuffled=[]
labels_shuffled=[]
for row,index in sentences:
  sentences_shuffled.append(row)
  labels_shuffled.append(labels[index])

print(len(sentences_shuffled),len(labels_shuffled),)
print(len(sentences),len(labels),sum(labels))

"""# Data Preprocessing using tokenizer and padding"""

import tensorflow as tf
import numpy as np 
from tensorflow.keras.preprocessing.text  import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

sentence_tokenizer=tf.keras.preprocessing.text.Tokenizer(oov_token="<oov>")
sentence_tokenizer.fit_on_texts(sentences_shuffled)

word_dict=sentence_tokenizer.word_index
word_dict

len(word_dict)

from matplotlib import pyplot as plt

sequences=sentence_tokenizer.texts_to_sequences(sentences_shuffled)
length=[len(seq) for seq in sequences]

length.sort()
input_length_max=max(length)
avg_length=sum(length)/(len(length))
print(input_length_max,avg_length)

length_sequence_section=[]
for i in range(1,11):
  idx=(len(length)-1)*i//10
  length_sequence_section.append(length[idx])
plt.plot([i for i in range(1,11)],length_sequence_section)

max_length=50

padded_sequence=tf.keras.preprocessing.sequence.pad_sequences(sequences,maxlen=max_length,padding="pre",truncating="pre")
labels=np.array(labels)

training_seq=padded_sequence[:int(len(padded_sequence)*.85)]
test_seq=padded_sequence[int(len(padded_sequence)*.85):]
training_label=labels_shuffled[:int(len(padded_sequence)*.85)]
test_label=labels_shuffled[int(len(padded_sequence)*.85):]

"""# Callbacks For tuning hyperparameters and early stopping"""

class StopCallBack(tf.keras.callbacks.Callback):

  def __init__(self,validation,training):
    self.validation=validation
    self.training=training

  def on_epoch_end(self,epochs,logs={}):
    if logs.get("accuracy")>=self.training and logs.get("val_accuracy")>=self.validation:
      self.model.stop_training=True

learning_scheduler_calback=tf.keras.callbacks.LearningRateScheduler(lambda epochs: 1e-8 * 10**(epochs/20))

"""# Model Creation"""

def creat_model():
  embedding_dims=16
  vocab_size=len(word_dict)+1
  model=tf.keras.Sequential([
  tf.keras.layers.Embedding(vocab_size, embedding_dims,input_shape=(max_length,)),
  tf.keras.layers.Dropout(.6),
  # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,return_sequences=True)),
  # tf.keras.layers.Dropout(.5),
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
  tf.keras.layers.Dense(32,activation="relu"),
  tf.keras.layers.Dropout(.6),
  tf.keras.layers.Dense(16,activation="relu"),
  tf.keras.layers.Dropout(.6),
  tf.keras.layers.Dense(1,activation="sigmoid")])
  return model

print(sum(labels))
print(sum(training_label))
print(sum(test_label))

training_label=np.array(training_label)
test_label=np.array(test_label)

model=creat_model()
model.summary()

model.compile(loss="binary_crossentropy",optimizer="adam",metrics=["accuracy"])

history=model.fit(training_seq,training_label,epochs=50,batch_size=512,validation_data=(test_seq,test_label))

val_accuracy=history.history["val_accuracy"]
accuracy=history.history["accuracy"]
loss=history.history["loss"]

plt.plot(val_accuracy)
plt.plot(accuracy)
plt.plot(loss)

model_one_lstm=tf.keras.Sequential([
    tf.keras.layers.Embedding(len(word_dict)+1,10,input_length=50),
    tf.keras.layers.LSTM(2),
    tf.keras.layers.Dropout(.6),
    tf.keras.layers.Dense(1,activation="sigmoid")
])

model_one_lstm.summary()

optimizer=tf.keras.optimizers.experimental.RMSprop(learning_rate=0.001)
model_one_lstm.compile(loss="binary_crossentropy",metrics=["accuracy"],optimizer="adam")

history=model_one_lstm.fit(training_seq,training_label,epochs=100,batch_size=128,validation_data=(test_seq,test_label))

val_accuracy=history.history["val_accuracy"]
accuracy=history.history["accuracy"]
loss=history.history["loss"]
plt.plot(val_accuracy)
plt.plot(accuracy)
plt.plot(loss)

def create_model_with_conv1D(vocab_size=100,embedding_dims=100,max_len=100):
  inputs=tf.keras.layers.Input(shape=(50,))
  x=tf.keras.layers.Embedding(len(word_dict)+1,100,input_length=max_len)(inputs)
  x=tf.keras.layers.Conv1D(32,4,activation="relu")(x)
  x=tf.keras.layers.GlobalAveragePooling1D()(x)
  x=tf.keras.layers.Dropout(.5)(x)

  # deleted the layers below because the dimensions didn't match
  # we can use lambda layer to expand the dims so that it matches the expected dims

  # x=tf.keras.layers.Conv1D(128,4,activation="relu")(x)
  # x=tf.keras.layers.GlobalAveragePooling1D()(x)
  # x=tf.keras.layers.Dropout(.5)(x)

  x=tf.keras.layers.Dense(16,activation="relu")(x)
  x=tf.keras.layers.Dropout(.3)(x)
  x=tf.keras.layers.Dense(8,activation="relu")(x)
  outputs=tf.keras.layers.Dense(1,activation="sigmoid")(x)
  model=tf.keras.Model(inputs=inputs,outputs=outputs)
  return model

model_2=create_model_with_conv1D()
model_2.summary()

model_2.compile(loss="binary_crossentropy",optimizer="adam",metrics=["accuracy"])
history=model_2.fit(training_seq,training_label,epochs=50,batch_size=512,validation_data=(test_seq,test_label))

model_2.save("model_3.h5")

val_accuracy=history.history["val_accuracy"]
accuracy=history.history["accuracy"]
loss=history.history["loss"]
plt.plot(val_accuracy)
plt.plot(accuracy)
plt.plot(loss)

def creat_model_no_rnn():
  embedding_dims=16
  vocab_size=len(word_dict)+1
  inputs=tf.keras.layers.Input(shape=(50))
  x=tf.keras.layers.Embedding(vocab_size,embedding_dims,input_length=100)(inputs) #returns word embedding matrix for the input
  # insted of flatten averages over the columns
  x=tf.keras.layers.Dropout(.5)(x) 
  x=tf.keras.layers.GlobalAveragePooling1D()(x)
  x= tf.keras.layers.Dense(24, activation='relu',kernel_regularizer=tf.keras.regularizers.l2())(x)
  x=tf.keras.layers.Dropout(.5)(x) 
  outputs=tf.keras.layers.Dense(1, activation='sigmoid')(x)
  model=tf.keras.Model(inputs=inputs,outputs=outputs)
  return model

model_3=creat_model_no_rnn()
model_3.summary()
model_3.compile(loss="binary_crossentropy",optimizer="adam",metrics=["accuracy"])

history=model_3.fit(training_seq,training_label,epochs=50,batch_size=512,validation_data=(test_seq,test_label))
val_accuracy=history.history["val_accuracy"]
accuracy=history.history["accuracy"]
loss=history.history["loss"]
plt.plot(val_accuracy)
plt.plot(accuracy)
plt.plot(loss)

model_3.save("model_4.h5")

model_lstm = tf.keras.Sequential([
    tf.keras.layers.Embedding(len(word_dict)+1, 15, input_length=50),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(10)),
    tf.keras.layers.Dropout(.6),
    tf.keras.layers.Dense(8, activation='relu',kernel_regularizer=tf.keras.regularizers.l2()),
    tf.keras.layers.Dropout(.5),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model_lstm.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

model_lstm.summary()

history=model_lstm.fit(training_seq,training_label,epochs=50,batch_size=512,validation_data=(test_seq,test_label))
val_accuracy=history.history["val_accuracy"]
accuracy=history.history["accuracy"]
loss=history.history["loss"]
plt.plot(val_accuracy)
plt.plot(accuracy)
plt.plot(loss)

model_lstm.save("")